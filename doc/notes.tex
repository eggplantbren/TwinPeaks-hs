\documentclass[a4paper, 12pt]{article}
\usepackage{amsmath}
\usepackage{color}
\usepackage{dsfont}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[left=2cm, right=2cm, bottom=3cm, top=2cm]{geometry}
\usepackage{natbib}
\usepackage{microtype}

\definecolor{orange}{rgb}{1, 0.5, 0}
\definecolor{darkgreen}{rgb}{0, 0.5, 0}
\definecolor{darkred}{rgb}{0.7, 0, 0}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\tn}{\textnormal}

\title{Notes}
\author{Brendon J. Brewer}
\date{}
\begin{document}
\maketitle

% Need this after the abstract
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\section*{A property of Nested Sampling}
Consider the implied prior for $L$, denoted $\pi(L)$. This is
a probability distribution over the real line.
The NS sequence rectifies $\pi$ to give you the distribution
\begin{align}
p_{\rm NS}(L) &= \frac{1}{C} \frac{\pi(L)}{X(L)}
\end{align}
and
\begin{align}
C &= \int_{-\infty}^{L_{\rm max}} \frac{\pi(L)}{X(L)} \, dL
\end{align}
where $L_{\rm max}$ is the maximum likelihood from the
discarded points of the run
(not necessarily the overall maximum likelihood).
The amount of prior mass above any specified likelihood value
$\ell$ is
\begin{align}
X(\ell) &= \int_{\ell}^{\infty} \pi(L) \, dL 
\end{align}
which is the normalisation constant of a constrained prior with
threshold $\ell$.
NS gives us the opportunity to measure this for any given
value of $\ell$. Consider the KL divergence from $p_{\rm NS}$ to
the constrained prior:
\begin{align}
D_{\rm KL}(p_\ell \,||\, p_{\rm NS})
  &= \int_{\ell}^{\infty} \frac{\pi(L)}{X(\ell)}
           \log\left[\frac{\pi(L)/X(\ell)}{\pi(L)/C/X(L)}\right] \, dL \\
  &= \int_{\ell}^{\infty} \frac{\pi(L)}{X(\ell)}
           \log\left[\frac{CX(L)}{X(\ell)}\right] \, dL
\end{align}
We can take the expectation (integral) in terms of $X$ instead of
$L$, and use the fact that the prior corresponds to a
Uniform$(0,1)$ distribution for $X$:
\begin{align}
D_{\rm KL}(p_\ell \,||\, p_{\rm NS})
    &= \int_0^{X(\ell)} \frac{1}{X(\ell)}
           \log(X) \, dX - \log X(\ell) + \log C \\
    &= \frac{1}{X(\ell)} \int_0^{X(\ell)} \log(X) \, dX
           - \log X(\ell) + \log C \\
    &= \frac{1}{X(\ell)}\left[X\log(X) - X\right]_0^{X(\ell)}
           - \log X(\ell) + \log C \\
    &= \log X(\ell) - 1 - \log X(\ell) + \log C \\
    &= \log C - 1.
\end{align}
Crucially, {\em this does not depend on} $\ell$.

\section*{TwinPeaks}
Denote the two scalars by $L_1(\btheta)$ and $L_2(\btheta)$.
The implied prior for $L_1$ and $L_2$ is $\pi(L_1, L_2)$.
Consider a constrained distribution
\begin{align}
p(L_1, L_2 \,|\, \ell_1, \ell_2) &\propto
    \left\{
        \begin{array}{lr}
            \pi(L_1, L_2),  &   L_1 > \ell_1 \tn{ and } L_2 > \ell_2 \\
            0,              &   \tn{otherwise}.
        \end{array}
    \right.
\end{align}
The normalising constant of this distribution is
\begin{align}
X(\ell_1, \ell_2) &=
    \int_{\ell_1}^{\infty}
    \int_{\ell_2}^{\infty}
        \pi(L_1, L_2)
    \, dL_1
    \, dL_2.
\end{align}

My proposed TwinPeaks distribution is
\begin{align}
p_{\rm TP}(L_1, L_2) &=
  \frac{1}{C}
  \frac{\pi(L_1, L_2)}{X(L_1, L_2)}.
\end{align}
As in the previous section, we can compute the KL divergence from
this to a constrained prior:
\begin{align}
D_{\rm KL}(p_{\ell_1, \ell_2} \,||\, p_{\rm TP})
  &= \int_{\ell_1}^{\infty}
     \int_{\ell_2}^{\infty}
         \frac{\pi(L_1, L_2)}{X(\ell_1, \ell_2)}
         \log\left[C \frac{X(L_1, L_2)}{X(\ell_1, \ell_2)} \right]
     \, dL_1
     \, dL_2 \\
  &=
     \int_{\ell_1}^{\infty}
     \int_{\ell_2}^{\infty}
         \frac{\pi(L_1, L_2)}{X(\ell_1, \ell_2)}
         \log\left[C \frac{X(L_1, L_2)}{X(\ell_1, \ell_2)} \right]
     \, dL_1
     \, dL_2 \\
  &=
     \log C \,-\, \log X(\ell_1, \ell_2) \\ &\quad\quad\quad
     \,+\,
     \frac{1}{X(\ell_1, \ell_2)}
     \int_{\ell_1}^{\infty}
     \int_{\ell_2}^{\infty}
         \pi(L_1, L_2)
         \log X(L_1, L_2)
     \, dL_1
     \, dL_2 \\
\end{align}
Doing the integral in terms of $X$


\end{document}

