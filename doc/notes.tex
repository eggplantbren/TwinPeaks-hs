\documentclass[a4paper, 12pt]{article}
\usepackage{amsmath}
\usepackage{color}
\usepackage{dsfont}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[left=2cm, right=2cm, bottom=3cm, top=2cm]{geometry}
\usepackage{natbib}
\usepackage{microtype}

\definecolor{orange}{rgb}{1, 0.5, 0}
\definecolor{darkgreen}{rgb}{0, 0.5, 0}
\definecolor{darkred}{rgb}{0.7, 0, 0}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\tn}{\textnormal}

\title{Notes}
\author{Brendon J. Brewer}
\date{}
\begin{document}
\maketitle

% Need this after the abstract
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\section{A property of Nested Sampling}
Consider the implied prior for $L$, denoted $\pi(L)$. This is
a probability distribution over the real line.
As usual, define $X(\ell)$ as the amount of prior mass
with likelihood above $\ell$:
\begin{align}
X(\ell) &= \int_\ell^\infty \pi(L) \, dL.
\end{align}
This is the complementary CDF of $L$.

The NS sequence rectifies $\pi$ to give you the distribution
\begin{align}
p_{\rm NS}(L) &= \frac{1}{C} \frac{\pi(L)}{X(L)}
\end{align}
where
\begin{align}
C &= \int_{-\infty}^{L_{\rm max}} \frac{\pi(L)}{X(L)} \, dL
\end{align}
is a normalising constant,
and $L_{\rm max}$ is the maximum likelihood from the
discarded points of the run
(not necessarily the overall maximum likelihood).

Consider a `constrained prior' distribution, which is just the
prior $\pi$ but truncated so it only includes values above
a threshold $\ell$:
\begin{align}
p(L \,|\, \ell) &\propto
               \left\{
                 \begin{array}{lr}
                   \pi(L),&  L > \ell, \\
                   0,     &  \tn{otherwise}.
                 \end{array}
               \right.
\end{align}
The normalisation constant of a constrained prior is
just $X(\ell)$.

NS gives us the opportunity to measure this for any given
value of $\ell$. Consider the KL divergence from $p_{\rm NS}$ to
the constrained prior:
\begin{align}
D_{\rm KL}(p_\ell \,||\, p_{\rm NS})
  &= \int_{\ell}^{\infty} \frac{\pi(L)}{X(\ell)}
           \log\left[\frac{\pi(L)/X(\ell)}{\pi(L)/C/X(L)}\right] \, dL \\
  &= \int_{\ell}^{\infty} \frac{\pi(L)}{X(\ell)}
           \log\left[\frac{CX(L)}{X(\ell)}\right] \, dL
\end{align}
We can take the expectation (integral) in terms of $X$ instead of
$L$, and use the fact that the prior corresponds to a
Uniform$(0,1)$ distribution for $X$:
\begin{align}
D_{\rm KL}(p_\ell \,||\, p_{\rm NS})
    &= \int_0^{X(\ell)} \frac{1}{X(\ell)}
           \log(X) \, dX - \log X(\ell) + \log C \\
    &= \frac{1}{X(\ell)} \int_0^{X(\ell)} \log(X) \, dX
           - \log X(\ell) + \log C \\
    &= \frac{1}{X(\ell)}\left[X\log(X) - X\right]_0^{X(\ell)}
           - \log X(\ell) + \log C \\
    &= \log X(\ell) - 1 - \log X(\ell) + \log C \\
    &= \log C - 1.
\end{align}
Crucially, {\em this does not depend on} $\ell$.

\section{Uniqueness}
Is $p_{\rm NS}$ the only distribution with the above property?
Consider the KL divergence from some distribution $q$
to the constrained prior.
We will then see what choices of $q$ have the above property.
The KL divergence is
\begin{align}
D_{\rm KL}(p_\ell \,||\, q)
  &= \int_{\ell}^{\infty} \frac{\pi(L)}{X(\ell)}
           \log\left[\frac{\pi(L)/X(\ell)}{q(L)}\right] \, dL \\
  &= -\log X(\ell) + \frac{1}{X(\ell)}
    \int_{\ell}^{\infty} \pi(L)
               \log\left[\frac{\pi(L)}{q(L)}\right] \, dL
\end{align}

The derivative of the KL divergence, with respect to $\ell$, is
\begin{align}
\frac{d}{d\ell} D_{\rm KL}(p_\ell \,||\, q)
  &= -\frac{X'(\ell)}{X(\ell)}
     + \frac{X(\ell)\pi(\ell)\log\left[\pi(\ell)/q(\ell)\right]
             -X'(\ell)\int_{\ell}^{\infty} \pi(L)
               \log\left[\pi(L)/q(L)\right] \, dL}
            {X(\ell)^2} \\
  &= -\frac{X'(\ell)}{X(\ell)}
     + \frac{\pi(\ell)\log\left[\pi(\ell)/q(\ell)\right]}
            {X(\ell)}
     - \frac{X'(\ell)/X(\ell)\int_{\ell}^{\infty} \pi(L)
               \log\left[\pi(L)/q(L)\right] \, dL}
            {X(\ell)}\\
  &= -\frac{X'(\ell)}{X(\ell)}
     + \frac{\pi(\ell)\log\left[\pi(\ell)/q(\ell)\right]}
            {X(\ell)}
     - \frac{X'(\ell)\left(D_{\rm KL} + \log X(\ell)\right)}
            {X(\ell)} \\
  &= \frac{\pi(\ell)}{X(\ell)}
        \Big(1
             + \log\left[\pi(\ell)/q(\ell)\right]
             + D_{\rm KL} + \log X(\ell)
        \Big)
\end{align}
To satisfy the nice property of the previous section, the derivative
must be zero. This is achieved if
\begin{align}
1 + \log\left[\pi(\ell)/q(\ell)\right]
+ D_{\rm KL} + \log X(\ell) &= 0 \\
1 + \log\pi(\ell) - \log q(\ell)
+ D_{\rm KL} + \log X(\ell) &= 0
\end{align}


%\section*{TwinPeaks}
%Denote the two scalars by $L_1(\btheta)$ and $L_2(\btheta)$.
%The implied prior for $L_1$ and $L_2$ is $\pi(L_1, L_2)$.
%Consider a constrained distribution
%\begin{align}
%p(L_1, L_2 \,|\, \ell_1, \ell_2) &\propto
%    \left\{
%        \begin{array}{lr}
%            \pi(L_1, L_2),  &   L_1 > \ell_1 \tn{ and } L_2 > \ell_2 \\
%            0,              &   \tn{otherwise}.
%        \end{array}
%    \right.
%\end{align}
%The normalising constant of this distribution is
%\begin{align}
%X(\ell_1, \ell_2) &=
%    \int_{\ell_1}^{\infty}
%    \int_{\ell_2}^{\infty}
%        \pi(L_1, L_2)
%    \, dL_1
%    \, dL_2.
%\end{align}

%My proposed TwinPeaks distribution is
%\begin{align}
%p_{\rm TP}(L_1, L_2) &=
%  \frac{1}{C}
%  \frac{\pi(L_1, L_2)}{X(L_1, L_2)}.
%\end{align}
%As in the previous section, we can compute the KL divergence from
%this to a constrained prior:
%\begin{align}
%D_{\rm KL}(p_{\ell_1, \ell_2} \,||\, p_{\rm TP})
%  &= \int_{\ell_1}^{\infty}
%     \int_{\ell_2}^{\infty}
%         \frac{\pi(L_1, L_2)}{X(\ell_1, \ell_2)}
%         \log\left[C \frac{X(L_1, L_2)}{X(\ell_1, \ell_2)} \right]
%     \, dL_1
%     \, dL_2 \\
%  &=
%     \int_{\ell_1}^{\infty}
%     \int_{\ell_2}^{\infty}
%         \frac{\pi(L_1, L_2)}{X(\ell_1, \ell_2)}
%         \log\left[C \frac{X(L_1, L_2)}{X(\ell_1, \ell_2)} \right]
%     \, dL_1
%     \, dL_2 \\
%  &=
%     \log C \,-\, \log X(\ell_1, \ell_2) \\ &\quad\quad\quad
%     \,+\,
%     \frac{1}{X(\ell_1, \ell_2)}
%     \int_{\ell_1}^{\infty}
%     \int_{\ell_2}^{\infty}
%         \pi(L_1, L_2)
%         \log X(L_1, L_2)
%     \, dL_1
%     \, dL_2 \\
%\end{align}
%Doing the integral in terms of $X$


\end{document}

